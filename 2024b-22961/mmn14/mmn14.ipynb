{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2b3911-1b93-452c-85e1-61c8f1db351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33822a8b-c278-489a-8a35-5958f64bd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLayer(nn.Module):\n",
    "    \"\"\" \n",
    "    An implementation of a neural network layer that given an input X it does the following:\n",
    "    - Split X into two halves X1, X2\n",
    "    - Feed X1, X2 into the same LinearLayer yielding outputs Z1, Z2\n",
    "    - Feed Z1, Z2 into a ReLU layer yielding outputs Y1, Y2\n",
    "    - Concatenate Y=[Y1, Y2]\n",
    "    \"\"\"\n",
    "\n",
    "    # The number of input batches\n",
    "    input_batches: int\n",
    "\n",
    "    # The number of input features.\n",
    "    # Because we split the input to two halves, this is expected to be even.\n",
    "    input_features: int\n",
    "\n",
    "    # Determines the size of each split.\n",
    "    # It is basically input_features/2\n",
    "    split_size: int\n",
    "\n",
    "    # Determines the dimension to split by.\n",
    "    # It is basically 1 since 1 is the expected features dimension.\n",
    "    split_dim: int\n",
    "\n",
    "    def __init__(self, input_batches: int, input_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert input_batches > 0, \"expected input_batches to be positive\"\n",
    "        assert input_features % 2 == 0, \"expected input_features to be even\"\n",
    "\n",
    "        self.input_batches = input_batches\n",
    "        self.input_features = input_features\n",
    "\n",
    "        self.split_dim = 1\n",
    "        self.split_size = self.input_features // 2\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(self.split_size, self.split_size),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Weight Initialization using Xavier's method.\n",
    "\n",
    "        As explained here\n",
    "        - https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization\n",
    "        \n",
    "        Xavier's method tackles the issue of exploding/vanishing gradients \n",
    "        \"\"\"\n",
    "        # sqrt(6 / (split_size + split_size))\n",
    "        xavier = math.sqrt(3 / self.split_size) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.hidden[0].weight.uniform_(-xavier, +xavier)\n",
    "\n",
    "        # Same as: (in fact I saw how to manually do it there)\n",
    "        # torch.nn.init.uniform_(self.hidden[0].weight, -xavier, +xavier)\n",
    "\n",
    "        # Bias is kept as is (0 bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, verbose=False):\n",
    "        log = print if verbose else lambda x: ()\n",
    "\n",
    "        log(f\"  Input: {x}\")\n",
    "\n",
    "        a_in, b_in = x.split(split_size=self.split_size, dim=self.split_dim)\n",
    "        log(f\"  Split A: {a_in}\")\n",
    "        log(f\"  Split B: {b_in}\")\n",
    "\n",
    "        a_out = self.hidden(a_in)\n",
    "        log(f\"  Ouput A: {a_in}\")\n",
    "\n",
    "        b_out = self.hidden(b_in)\n",
    "        log(f\"  Output B: {b_in}\")\n",
    "\n",
    "        y = torch.concat((a_out, b_out), dim=self.split_dim)\n",
    "        log(f\"  Output: {y}\")\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37583035-659c-4c00-a660-ec1f16dfe23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration of SplitLayer: \n",
      "  Input: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "  Split A: tensor([[0., 1.],\n",
      "        [4., 5.],\n",
      "        [8., 9.]])\n",
      "  Split B: tensor([[ 2.,  3.],\n",
      "        [ 6.,  7.],\n",
      "        [10., 11.]])\n",
      "  Ouput A: tensor([[0., 1.],\n",
      "        [4., 5.],\n",
      "        [8., 9.]])\n",
      "  Output B: tensor([[ 2.,  3.],\n",
      "        [ 6.,  7.],\n",
      "        [10., 11.]])\n",
      "  Output: tensor([[ 0.3052,  1.7497,  3.3548,  1.6828],\n",
      "        [ 6.4043,  1.6159,  9.4539,  1.5490],\n",
      "        [12.5035,  1.4821, 15.5530,  1.4152]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_split_layer():\n",
    "    print(\"Demonstration of SplitLayer: \")\n",
    "    \n",
    "    x = torch.arange(12).reshape((3, 4)).float()\n",
    "    \n",
    "    split_layer = SplitLayer(3, 4)\n",
    "    split_layer.train(False)\n",
    "    \n",
    "    split_layer.forward(x, verbose=True)\n",
    "\n",
    "demonstrate_split_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0fc2e-8ed1-487f-8351-6a3de99580f2",
   "metadata": {},
   "source": [
    "## SplitLayer illustration\n",
    "\n",
    "![title](SplitLayer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b29774-dc13-4c1a-8556-6a99a0a07b14",
   "metadata": {},
   "source": [
    "## Parameters size\n",
    "\n",
    "A linear layer that maps an $n$ dimensional vector to an $m$ dimensional vector is a matrix of size $n \\times m$ and a bias vector of size $m$.\n",
    "\n",
    "Thus\n",
    "- To map the original input $X$ of size $M$ we require $M^2 + M$ parameters.\n",
    "- To map a splitted input of $X$ which is of size $\\frac{M}{2}$ we require $\\frac{M^2}{4} + \\frac{M}{2}$ parameters\n",
    "\n",
    "Because we feed both splits to the same layer, we use the same parameters and do not instantiate them twice.\n",
    "\n",
    "To conclude, _SplitLayer_ uses (asymptotically) a quarter of the number of the parameters used by _LinearLayer_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484aa94-4a27-4cf0-acbc-5d3e9fb9af6a",
   "metadata": {},
   "source": [
    "## Gradient Calculation for 2-split\n",
    "\n",
    "Denote by $W$ and $b$ the parameters of the _Linear_ layer.\n",
    "\n",
    "Then, by the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W} = \\frac{\\partial C}{\\partial Y} \\cdot \\begin{bmatrix}\n",
    "    \\frac{\\partial Y_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W} \\\\\n",
    "    \\frac{\\partial Y_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial W} \\\\\n",
    "\\end{bmatrix}\n",
    "~~;~~\n",
    "\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial Y} \\cdot \\begin{bmatrix}\n",
    "    \\frac{\\partial Y_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial b} \\\\\n",
    "    \\frac{\\partial Y_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial b} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By definition of the _ReLU_ layer $ReLU(x) = \\max \\{x, 0 \\}$ we get the derivatives\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_i}{\\partial Z_i} = \\delta (Z_i)\n",
    "$$\n",
    "\n",
    "where $\\delta$ is an elementwise function s.t $\\delta(X) = [\\delta(x_i)]$ and the scalar $\\delta(x_i)$ is defined by\n",
    "\n",
    "$$\n",
    "\\delta(x_i) = \\begin{cases}\n",
    "    1 & x_i > 0 \\\\\n",
    "    0 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By definition of the _Linear_ layer $Z_i = X_i W + b$ we get the derivatives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Z_i}{W} = X_i ~~;~~ \\frac{\\partial Z_i}{b} = 1\n",
    "$$\n",
    "\n",
    "Denoting the scalar $\\frac{\\partial C}{\\partial Y} = c$ and putting it all together we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W} = c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\cdot X_1 \\\\\n",
    "    \\delta(Z_2) \\cdot X_2 \\\\\n",
    "\\end{bmatrix}\n",
    "~~;~~\n",
    "\\frac{\\partial C}{\\partial b} = c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\\\\n",
    "    \\delta(Z_2) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "i.e an optimization step with learning rate $\\alpha$ is done by\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\alpha c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\cdot X_1 \\\\\n",
    "    \\delta(Z_2) \\cdot X_2 \\\\\n",
    "\\end{bmatrix}\n",
    "~~;~~\n",
    "b \\leftarrow b - \\alpha c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\\\\n",
    "    \\delta(Z_2) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e9c8c-3d8b-4cd6-a590-0e535c3e67df",
   "metadata": {},
   "source": [
    "## Extension of the Gradient for 4-split\n",
    "\n",
    "The benefit of presenting the derivatives in vector form is that it is clearly generalized.\n",
    "\n",
    "For an input the is split to 4 components $X_1, X_2, X_3$ and $X_4$ we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W} = c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\cdot X_1 \\\\\n",
    "    \\delta(Z_2) \\cdot X_2 \\\\\n",
    "    \\delta(Z_3) \\cdot X_3 \\\\\n",
    "    \\delta(Z_4) \\cdot X_4 \\\\\n",
    "\\end{bmatrix}\n",
    "~~;~~\n",
    "\\frac{\\partial C}{\\partial b} = c \\cdot \\begin{bmatrix}\n",
    "    \\delta(Z_1) \\\\\n",
    "    \\delta(Z_2) \\\\\n",
    "    \\delta(Z_3) \\\\\n",
    "    \\delta(Z_4) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
