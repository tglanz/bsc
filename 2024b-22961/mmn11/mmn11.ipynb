{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b14a557-2715-44aa-bf83-77336e451a8c",
   "metadata": {},
   "source": [
    "# 22961, Mmn11\n",
    "\n",
    "Author: Tal Glanzman\n",
    "\n",
    "Source: [Github](https://github.com/tglanz/bsc/blob/master/2024b-22961/mmn11/mmn11.ipynb)\n",
    "\n",
    "Below we implement the functions `expand`, `are_broadcastable_together` and `broadcast_tensors`. Under each of those functions, there is a simple tester function that evaluate the implementation against the equivalent torch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad872bea-ce1f-44c6-bcfa-34021af251ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa52c3c1-01b8-40a8-a48d-607e69945878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(tensor: torch.Tensor, size: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expand a given `tensor`, to a newly allocated tensor, with the shape given by `size`.\n",
    "    \n",
    "    The expansion is done by the broadcast rules:\n",
    "    - As long as the rank of `tensor` is less than `len(size)`\n",
    "      - Add degenerative axis to the tensor\n",
    "    - Iterating over every dimesnion `dim`, right to left:\n",
    "      - If the `tensor.shape[dim] != size[dim]`:\n",
    "        - If `tensor.shape[dim] == 1`:\n",
    "          - Concatenate the tensor with itself, `shape[dim]` times, along the `dim`th axis\n",
    "        - Else: An error is raised indicating that the tensor is not expandable to size `size`\n",
    "\n",
    "    Arguments\n",
    "      tensor {torch.Tensor} The tensor to expand\n",
    "      size {torch.Size} The size to expand the tensor to\n",
    "\n",
    "    Returns\n",
    "      {torch.Tensor} A newly allcoated tensor\n",
    "    \"\"\"\n",
    "    ans = tensor.clone()\n",
    "    rank = len(size)\n",
    "    \n",
    "    # Iterate on every (to) dimension\n",
    "    for i in range(rank):\n",
    "\n",
    "        # A cursor to the current axis in `size`\n",
    "        j = rank - 1 - i\n",
    "\n",
    "        # A cursor to the current dimension in `tensor.shape`.\n",
    "        # Note that it can be negative at first, but at most -1.\n",
    "        k = len(ans.shape) - 1 - i\n",
    "\n",
    "        if k < 0:\n",
    "            # It is sufficient to only add one degenerate dimension because we\n",
    "            # add them one by one - so we know that the index will be in bounds.\n",
    "            ans = ans.unsqueeze(0)\n",
    "            k = 0\n",
    "\n",
    "        ans_dimsize = ans.shape[k]\n",
    "        dimsize = size[j]\n",
    "\n",
    "        if ans_dimsize != dimsize:\n",
    "            if ans_dimsize != 1:\n",
    "                raise Exception(f\"Cannot expand tensor with shape {tuple(ans.shape)} to shape {size}\")\n",
    "            ans = torch.cat([ans] * dimsize, dim=k)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74ca875-db11-442c-a0c5-9e04be208451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test 0 =======\n",
      "tensor_shape (1,)\n",
      "size (3,)\n",
      "\n",
      "====== Test 1 =======\n",
      "tensor_shape (1,)\n",
      "size (2, 4, 5)\n",
      "\n",
      "====== Test 2 =======\n",
      "tensor_shape (3, 2)\n",
      "size (1, 1, 3, 2)\n",
      "\n",
      "====== Test 3 =======\n",
      "tensor_shape (1, 3, 2, 1, 1)\n",
      "size (1, 3, 2)\n",
      "Torch failed to expand\n",
      "Custom implementation failed to expand\n",
      "\n",
      "====== Test 4 =======\n",
      "tensor_shape (2,)\n",
      "size (3,)\n",
      "Torch failed to expand\n",
      "Custom implementation failed to expand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_expand():\n",
    "    \"\"\"Tester for `are_broadcastable_together`\"\"\"\n",
    "    \n",
    "    testcases = [\n",
    "        ((1, ), (3, )),\n",
    "        ((1, ), (2, 4, 5)),\n",
    "        ((3, 2), (1, 1, 3, 2)),\n",
    "        ((1, 3, 2, 1, 1), (1, 3, 2)),\n",
    "        ((2,), (3,))\n",
    "    ]\n",
    "    \n",
    "    for i, (tensor_shape, size) in enumerate(testcases):\n",
    "        print(f\"====== Test {i} =======\")\n",
    "        tensor = torch.rand(tensor_shape)\n",
    "\n",
    "        print(\"tensor_shape\", tensor_shape)\n",
    "        print(\"size\", size)\n",
    "\n",
    "        torch_error = False\n",
    "        custom_error = False\n",
    "        \n",
    "        try:\n",
    "            torch_expanded = tensor.expand(size)\n",
    "        except:\n",
    "            print(\"Torch failed to expand\")\n",
    "            torch_error = True\n",
    "\n",
    "        try:\n",
    "            custom_expanded = expand(tensor, size)\n",
    "        except:\n",
    "            print(\"Custom implementation failed to expand\")\n",
    "            custom_error = True\n",
    "\n",
    "        assert torch_error == custom_error, \"Only one of Torch and Custom implementations raised an error\"\n",
    "\n",
    "        if not torch_error:\n",
    "            assert torch.equal(torch_expanded, custom_expanded), \"Torch and Custom implementation yielded different tensors\"\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "test_expand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec85a4db-307b-439c-8c48-6025945a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_broadcastable_together(a: torch.Tensor, b: torch.Tensor) -> Tuple[bool, torch.Size]:\n",
    "    \"\"\"\n",
    "    Check whether two {torch.Tensor}s are broadcastable together.\n",
    "    If they do, the shared size is returned.\n",
    "\n",
    "    To acquire the shared size we (logically, not algorithmically) following this:\n",
    "    - While the ranks of `a` and `b` are not the same, add a degenerative dimension\n",
    "      as dimension 0 for the lower ranked tensor. \n",
    "    - For every dimension of `a` and `b`, `adim` and `bdim` respectively\n",
    "      - If `adim != bdim`\n",
    "        - If `adim == 1`: Set the corresponding dimension of the result size to `bdim`\n",
    "        - If `bdim == 1`: Set the corresponding dimension of the result size to `adim`\n",
    "        - Else: The tensors are not broadcastable together!\n",
    "\n",
    "    Returns\n",
    "        A tuple:\n",
    "            {boolean} - Indicates whether the tensors can be broadcastable together\n",
    "            {torch.Size} - If the first element is True, this size contains the shared\n",
    "                           size the tensors can be expanded to.\n",
    "    \"\"\"\n",
    "    shape_a = list(a.shape)\n",
    "    shape_b = list(b.shape)\n",
    "    \n",
    "    rank_a = len(shape_a)\n",
    "    rank_b = len(shape_b)\n",
    "\n",
    "    for i in range(max(rank_a, rank_b)):\n",
    "        axis_a = rank_a - 1 - i\n",
    "        axis_b = rank_b - 1 - i\n",
    "\n",
    "        if axis_a < 0:\n",
    "            shape_a.insert(0, 1)\n",
    "            axis_a = 0\n",
    "\n",
    "        if axis_b < 0:\n",
    "            shape_b.insert(0, 1)\n",
    "            axis_b = 0\n",
    "\n",
    "        size_a = shape_a[axis_a]\n",
    "        size_b = shape_b[axis_b]\n",
    "\n",
    "        if size_a != size_b:\n",
    "            if size_a == 1:\n",
    "                shape_a[axis_a] = size_b\n",
    "            elif size_b == 1:\n",
    "                shape_b[axis_b] = size_a\n",
    "            else:\n",
    "                return (False, None)\n",
    "\n",
    "    assert shape_a == shape_b, \"Should never happen - It's a bug if it did\"\n",
    "    return (True, tuple(shape_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ccc6030-7a48-4949-9aaf-490a892d45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test 0 ======\n",
      "a_shape (1,)\n",
      "b_shape (1, 1)\n",
      "Actual shape:  (1, 1)\n",
      "Torch shape:  (1, 1)\n",
      "\n",
      "====== Test 1 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (2, 1, 1, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 2 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (5, 1)\n",
      "Actual shape:  (2, 5, 2)\n",
      "Torch shape:  (2, 5, 2)\n",
      "\n",
      "====== Test 3 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (1, 5)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 4 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (1,)\n",
      "Actual shape:  (2, 1, 2)\n",
      "Torch shape:  (2, 1, 2)\n",
      "\n",
      "====== Test 5 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (5, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 6 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (5, 1)\n",
      "Actual shape:  (2, 1, 5, 3)\n",
      "Torch shape:  (2, 1, 5, 3)\n",
      "\n",
      "====== Test 7 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (1, 5)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 8 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (1,)\n",
      "Actual shape:  (2, 1, 1, 3)\n",
      "Torch shape:  (2, 1, 1, 3)\n",
      "\n",
      "====== Test 9 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (2, 1, 5, 3)\n",
      "Torch shape:  (2, 1, 5, 3)\n",
      "\n",
      "====== Test 10 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (1, 5)\n",
      "Actual shape:  (5, 5)\n",
      "Torch shape:  (5, 5)\n",
      "\n",
      "====== Test 11 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (1,)\n",
      "Actual shape:  (5, 1)\n",
      "Torch shape:  (5, 1)\n",
      "\n",
      "====== Test 12 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (5, 3)\n",
      "Torch shape:  (5, 3)\n",
      "\n",
      "====== Test 13 ======\n",
      "a_shape (1, 5)\n",
      "b_shape (1,)\n",
      "Actual shape:  (1, 5)\n",
      "Torch shape:  (1, 5)\n",
      "\n",
      "====== Test 14 ======\n",
      "a_shape (1, 5)\n",
      "b_shape (5, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 15 ======\n",
      "a_shape (1,)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (5, 3)\n",
      "Torch shape:  (5, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_are_broadcastable_together():\n",
    "    \"\"\"Tester for `are_broadcastable_together`\"\"\"\n",
    "    \n",
    "    testcases = [\n",
    "        ((1,), (1, 1)),\n",
    "\n",
    "        ((2, 1, 2), (2, 1, 1, 3)),\n",
    "        ((2, 1, 2), (5, 1)),\n",
    "        ((2, 1, 2), (1, 5)),\n",
    "        ((2, 1, 2), (1,)),\n",
    "        ((2, 1, 2), (5, 3)),\n",
    "\n",
    "        ((2, 1, 1, 3), (5, 1)),\n",
    "        ((2, 1, 1, 3), (1, 5)),\n",
    "        ((2, 1, 1, 3), (1,)),\n",
    "        ((2, 1, 1, 3), (5, 3)),\n",
    "\n",
    "        ((5, 1), (1, 5)),\n",
    "        ((5, 1), (1, )),\n",
    "        ((5, 1), (5, 3)),\n",
    "\n",
    "        ((1, 5), (1, )),\n",
    "        ((1, 5), (5, 3)),\n",
    "        ((1, ), (5, 3)),\n",
    "    ]\n",
    "\n",
    "    for i, (a_shape, b_shape) in enumerate(testcases):\n",
    "        print(f\"====== Test {i} ======\")\n",
    "        print(\"a_shape\", a_shape)\n",
    "        print(\"b_shape\", b_shape)\n",
    "        \n",
    "        a = torch.empty(a_shape)\n",
    "        b = torch.empty(b_shape)\n",
    "        \n",
    "        actual_broadcastable, actual_shape = are_broadcastable_together(a, b)\n",
    "\n",
    "        if not actual_broadcastable:\n",
    "            print(\"Custom implementation couldn't broadcast\")\n",
    "            \n",
    "        try:\n",
    "            torch_broadcast = torch.broadcast_tensors(a, b)\n",
    "        except:\n",
    "            print(\"Torch couldn't broadcast\\n\")\n",
    "            assert not actual_broadcastable, \"Torch couldn't broadcast but custom implemntation did\"\n",
    "            continue\n",
    "\n",
    "        torch_shape = tuple(torch_broadcast[0].shape)\n",
    "        print(\"Actual shape: \", actual_shape)\n",
    "        print(\"Torch shape: \", torch_shape)\n",
    "        \n",
    "        assert actual_shape == torch_shape, \"Torch and Custom implementation broadcasted differently\"\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "test_are_broadcastable_together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7fc96f-ebb3-488c-babe-641e715c3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_tensors(a: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Basically check and get the shared size using `are_broadcastable_together` and then\n",
    "    invoke `expand` on `a` and `b`.\n",
    "\n",
    "    Returns\n",
    "        Tuple of:\n",
    "            {torch.Tensor} the expanded tensor emerged from `a`\n",
    "            {torch.Tensor} the expanded tensor emerged from `b`\n",
    "    \"\"\"\n",
    "    broadcastable, size = are_broadcastable_together(a, b)\n",
    "    if not broadcastable:\n",
    "        raise Exception(\n",
    "            f'Unable to broadcast tensors of shapes \"{a.shape}\" and \"{b.shape}\"')\n",
    "\n",
    "    return (\n",
    "        expand(a, size),\n",
    "        expand(b, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6992d85c-8275-477f-a51c-7b550c8c2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.8659])\n",
      "b tensor([[0.6917]])\n",
      "a tensor([[[0.6398, 0.4981]],\n",
      "\n",
      "        [[0.5068, 0.3843]]])\n",
      "b tensor([[[[0.5536, 0.3150, 0.6765]]],\n",
      "\n",
      "\n",
      "        [[[0.0698, 0.7538, 0.7240]]]])\n",
      "a tensor([[[0.0488, 0.3205]],\n",
      "\n",
      "        [[0.0081, 0.1665]]])\n",
      "b tensor([[0.9793],\n",
      "        [0.1753],\n",
      "        [0.7702],\n",
      "        [0.2481],\n",
      "        [0.4139]])\n",
      "a tensor([[[0.2039, 0.4526]],\n",
      "\n",
      "        [[0.6042, 0.9553]]])\n",
      "b tensor([[0.1932, 0.3652, 0.3448, 0.6460, 0.3998]])\n",
      "a tensor([[[0.7531, 0.4035]],\n",
      "\n",
      "        [[0.4709, 0.6210]]])\n",
      "b tensor([0.7148])\n",
      "a tensor([[[0.7962, 0.3923]],\n",
      "\n",
      "        [[0.4812, 0.3860]]])\n",
      "b tensor([[0.1893, 0.8689, 0.8801],\n",
      "        [0.2783, 0.8493, 0.7404],\n",
      "        [0.1572, 0.1351, 0.1475],\n",
      "        [0.9143, 0.9257, 0.4259],\n",
      "        [0.0225, 0.8703, 0.1984]])\n",
      "a tensor([[[[0.2183, 0.9953, 0.0589]]],\n",
      "\n",
      "\n",
      "        [[[0.4218, 0.0820, 0.6053]]]])\n",
      "b tensor([[0.3629],\n",
      "        [0.1418],\n",
      "        [0.6754],\n",
      "        [0.7112],\n",
      "        [0.0321]])\n",
      "a tensor([[[[0.4140, 0.1143, 0.8713]]],\n",
      "\n",
      "\n",
      "        [[[0.9913, 0.8979, 0.5478]]]])\n",
      "b tensor([[0.1125, 0.1285, 0.6269, 0.5865, 0.5064]])\n",
      "a tensor([[[[0.8301, 0.9885, 0.8400]]],\n",
      "\n",
      "\n",
      "        [[[0.0089, 0.2888, 0.5141]]]])\n",
      "b tensor([0.9701])\n",
      "a tensor([[[[0.8298, 0.7496, 0.1956]]],\n",
      "\n",
      "\n",
      "        [[[0.8691, 0.6678, 0.3609]]]])\n",
      "b tensor([[0.5057, 0.8656, 0.2333],\n",
      "        [0.6485, 0.9034, 0.7150],\n",
      "        [0.8283, 0.0561, 0.1140],\n",
      "        [0.4156, 0.7414, 0.3604],\n",
      "        [0.4791, 0.8524, 0.7794]])\n",
      "a tensor([[0.3425],\n",
      "        [0.3198],\n",
      "        [0.4638],\n",
      "        [0.1284],\n",
      "        [0.0742]])\n",
      "b tensor([[0.0696, 0.6214, 0.2666, 0.4101, 0.7394]])\n",
      "a tensor([[0.0563],\n",
      "        [0.4121],\n",
      "        [0.6167],\n",
      "        [0.7093],\n",
      "        [0.6981]])\n",
      "b tensor([0.0697])\n",
      "a tensor([[0.3506],\n",
      "        [0.9544],\n",
      "        [0.9040],\n",
      "        [0.9314],\n",
      "        [0.8637]])\n",
      "b tensor([[0.3187, 0.2160, 0.7639],\n",
      "        [0.9885, 0.7393, 0.1196],\n",
      "        [0.7482, 0.6381, 0.7156],\n",
      "        [0.1378, 0.0979, 0.7637],\n",
      "        [0.2251, 0.0074, 0.7727]])\n",
      "a tensor([[0.8157, 0.8579, 0.1094, 0.7696, 0.4777]])\n",
      "b tensor([0.0686])\n",
      "a tensor([[0.2504, 0.5886, 0.9026, 0.8060, 0.8838]])\n",
      "b tensor([[0.8765, 0.0329, 0.3198],\n",
      "        [0.9519, 0.5656, 0.0096],\n",
      "        [0.8018, 0.6744, 0.5742],\n",
      "        [0.5632, 0.4848, 0.6605],\n",
      "        [0.2227, 0.7401, 0.4351]])\n",
      "a tensor([0.3758])\n",
      "b tensor([[0.4085, 0.1607, 0.3779],\n",
      "        [0.7905, 0.9114, 0.1461],\n",
      "        [0.7581, 0.7829, 0.5246],\n",
      "        [0.7025, 0.1277, 0.6752],\n",
      "        [0.0360, 0.0802, 0.5905]])\n"
     ]
    }
   ],
   "source": [
    "def test_broadcast_tensors():\n",
    "    testcases = [\n",
    "        ((1,), (1, 1)),\n",
    "\n",
    "        ((2, 1, 2), (2, 1, 1, 3)),\n",
    "        ((2, 1, 2), (5, 1)),\n",
    "        ((2, 1, 2), (1, 5)),\n",
    "        ((2, 1, 2), (1,)),\n",
    "        ((2, 1, 2), (5, 3)),\n",
    "\n",
    "        ((2, 1, 1, 3), (5, 1)),\n",
    "        ((2, 1, 1, 3), (1, 5)),\n",
    "        ((2, 1, 1, 3), (1,)),\n",
    "        ((2, 1, 1, 3), (5, 3)),\n",
    "\n",
    "        ((5, 1), (1, 5)),\n",
    "        ((5, 1), (1, )),\n",
    "        ((5, 1), (5, 3)),\n",
    "\n",
    "        ((1, 5), (1, )),\n",
    "        ((1, 5), (5, 3)),\n",
    "        ((1, ), (5, 3)),\n",
    "    ]\n",
    "\n",
    "    for i, (a_shape, b_shape) in enumerate(testcases):\n",
    "        a = torch.rand(a_shape)\n",
    "        b = torch.rand(b_shape)\n",
    "\n",
    "        error_custom = False\n",
    "        error_torch = False\n",
    "\n",
    "        print(\"a\", a)\n",
    "        print(\"b\", b)\n",
    "\n",
    "        try:\n",
    "            torch_broadcast = torch.broadcast_tensors(a, b)\n",
    "        except:\n",
    "            error_torch = True\n",
    "\n",
    "        try:\n",
    "            custom_broadcast = broadcast_tensors(a, b)\n",
    "        except:\n",
    "            error_custom = True\n",
    "\n",
    "        assert error_torch == error_custom\n",
    "\n",
    "        if not error_torch:\n",
    "            assert torch.equal(torch_broadcast[0], custom_broadcast[0]), \"First tensors of Torch and Custom implementation are not the same\"\n",
    "            assert torch.equal(torch_broadcast[1], custom_broadcast[1]), \"Second tensors of Torch and Custom implementation are not the same\"\n",
    "\n",
    "test_broadcast_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678376d-6a3b-4839-b13e-ac02bdab759a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
