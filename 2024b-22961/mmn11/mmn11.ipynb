{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b14a557-2715-44aa-bf83-77336e451a8c",
   "metadata": {},
   "source": [
    "# 22961, Mmn11\n",
    "\n",
    "Author: Tal Glanzman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad872bea-ce1f-44c6-bcfa-34021af251ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa52c3c1-01b8-40a8-a48d-607e69945878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(tensor: torch.Tensor, size: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expand a given `tensor`, to a newly allocated tensor, with the shape given by `size`.\n",
    "    \n",
    "    The expansion is done by the broadcast rules:\n",
    "    - As long as the rank of `tensor` is less than `len(size)`\n",
    "      - Add degenerative axis to the tensor\n",
    "    - Iterating over every dimesnion `dim`, right to left:\n",
    "      - If the `tensor.shape[dim] != size[dim]`:\n",
    "        - If `tensor.shape[dim] == 1`:\n",
    "          - Concatenate the tensor with itself, `shape[dim]` times, along the `dim`th axis\n",
    "        - Else: An error is raised indicating that the tensor is not expandable to size `size`\n",
    "\n",
    "    Arguments\n",
    "      tensor {torch.Tensor} The tensor to expand\n",
    "      size {torch.Size} The size to expand the tensor to\n",
    "\n",
    "    Returns\n",
    "      {torch.Tensor} A newly allcoated tensor\n",
    "    \"\"\"\n",
    "    ans = tensor.clone()\n",
    "    rank = len(size)\n",
    "    \n",
    "    # Iterate on every (to) dimension\n",
    "    for i in range(rank):\n",
    "\n",
    "        # A cursor to the current axis in `size`\n",
    "        j = rank - 1 - i\n",
    "\n",
    "        # A cursor to the current dimension in `tensor.shape`.\n",
    "        # Note that it can be negative at first, but at most -1.\n",
    "        k = len(ans.shape) - 1 - i\n",
    "\n",
    "        if k < 0:\n",
    "            # It is sufficient to only add one degenerate dimension because we\n",
    "            # add them one by one - so we know that the index will be in bounds.\n",
    "            ans = ans.unsqueeze(0)\n",
    "            k = 0\n",
    "\n",
    "        ans_dimsize = ans.shape[k]\n",
    "        dimsize = size[j]\n",
    "\n",
    "        if ans_dimsize != dimsize:\n",
    "            if ans_dimsize != 1:\n",
    "                raise Exception(f\"Cannot expand tensor with shape {tuple(ans.shape)} to shape {size}\")\n",
    "            ans = torch.cat([ans] * dimsize, dim=k)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d74ca875-db11-442c-a0c5-9e04be208451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test 0 =======\n",
      "tensor_shape (1,)\n",
      "size (3,)\n",
      "\n",
      "====== Test 1 =======\n",
      "tensor_shape (1,)\n",
      "size (2, 4, 5)\n",
      "\n",
      "====== Test 2 =======\n",
      "tensor_shape (3, 2)\n",
      "size (1, 1, 3, 2)\n",
      "\n",
      "====== Test 3 =======\n",
      "tensor_shape (1, 3, 2, 1, 1)\n",
      "size (1, 3, 2)\n",
      "Torch failed to expand\n",
      "Custom implementation failed to expand\n",
      "\n",
      "====== Test 4 =======\n",
      "tensor_shape (2,)\n",
      "size (3,)\n",
      "Torch failed to expand\n",
      "Custom implementation failed to expand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_expand():\n",
    "    \"\"\"Tester for `are_broadcastable_together`\"\"\"\n",
    "    \n",
    "    testcases = [\n",
    "        ((1, ), (3, )),\n",
    "        ((1, ), (2, 4, 5)),\n",
    "        ((3, 2), (1, 1, 3, 2)),\n",
    "        ((1, 3, 2, 1, 1), (1, 3, 2)),\n",
    "        ((2,), (3,))\n",
    "    ]\n",
    "    \n",
    "    for i, (tensor_shape, size) in enumerate(testcases):\n",
    "        print(f\"====== Test {i} =======\")\n",
    "        tensor = torch.empty(tensor_shape)\n",
    "\n",
    "        print(\"tensor_shape\", tensor_shape)\n",
    "        print(\"size\", size)\n",
    "\n",
    "        torch_error = False\n",
    "        custom_error = False\n",
    "        \n",
    "        try:\n",
    "            torch_expanded = tensor.expand(size)\n",
    "        except:\n",
    "            print(\"Torch failed to expand\")\n",
    "            torch_error = True\n",
    "\n",
    "        try:\n",
    "            custom_expanded = expand(tensor, size)\n",
    "        except:\n",
    "            print(\"Custom implementation failed to expand\")\n",
    "            custom_error = True\n",
    "\n",
    "        assert torch_error == custom_error, \"Only one of Torch and Custom implementations raised an error\"\n",
    "\n",
    "        if not torch_error:\n",
    "            assert torch.equal(torch_expanded, custom_expanded), \"Torch and Custom implementation yielded different tensors\"\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "test_expand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec85a4db-307b-439c-8c48-6025945a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_broadcastable_together(a: torch.Tensor, b: torch.Tensor) -> Tuple[bool, torch.Size]:\n",
    "    \"\"\"\n",
    "    Check whether two {torch.Tensor}s are broadcastable together.\n",
    "    If they do, the shared size is returned.\n",
    "\n",
    "    To acquire the shared size we (logically, not algorithmically) following this:\n",
    "    - While the ranks of `a` and `b` are not the same, add a degenerative dimension\n",
    "      as dimension 0 for the lower ranked tensor. \n",
    "    - For every dimension of `a` and `b`, `adim` and `bdim` respectively\n",
    "      - If `adim != bdim`\n",
    "        - If `adim == 1`: Set the corresponding dimension of the result size to `bdim`\n",
    "        - If `bdim == 1`: Set the corresponding dimension of the result size to `adim`\n",
    "        - Else: The tensors are not broadcastable together!\n",
    "\n",
    "    Returns\n",
    "        A tuple:\n",
    "            {boolean} - Indicates whether the tensors can be broadcastable together\n",
    "            {torch.Size} - If the first element is True, this size contains the shared\n",
    "                           size the tensors can be expanded to.\n",
    "    \"\"\"\n",
    "    shape_a = list(a.shape)\n",
    "    shape_b = list(b.shape)\n",
    "    \n",
    "    rank_a = len(shape_a)\n",
    "    rank_b = len(shape_b)\n",
    "\n",
    "    for i in range(max(rank_a, rank_b)):\n",
    "        axis_a = rank_a - 1 - i\n",
    "        axis_b = rank_b - 1 - i\n",
    "\n",
    "        if axis_a < 0:\n",
    "            shape_a.insert(0, 1)\n",
    "            axis_a = 0\n",
    "\n",
    "        if axis_b < 0:\n",
    "            shape_b.insert(0, 1)\n",
    "            axis_b = 0\n",
    "\n",
    "        size_a = shape_a[axis_a]\n",
    "        size_b = shape_b[axis_b]\n",
    "\n",
    "        if size_a != size_b:\n",
    "            if size_a == 1:\n",
    "                shape_a[axis_a] = size_b\n",
    "            elif size_b == 1:\n",
    "                shape_b[axis_b] = size_a\n",
    "            else:\n",
    "                return (False, None)\n",
    "\n",
    "    assert shape_a == shape_b, \"Should never happen - It's a bug if it did\"\n",
    "    return (True, tuple(shape_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ccc6030-7a48-4949-9aaf-490a892d45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test 0 ======\n",
      "a_shape (1,)\n",
      "b_shape (1, 1)\n",
      "Actual shape:  (1, 1)\n",
      "Torch shape:  (1, 1)\n",
      "\n",
      "====== Test 1 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (2, 1, 1, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 2 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (5, 1)\n",
      "Actual shape:  (2, 5, 2)\n",
      "Torch shape:  (2, 5, 2)\n",
      "\n",
      "====== Test 3 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (1, 5)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 4 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (1,)\n",
      "Actual shape:  (2, 1, 2)\n",
      "Torch shape:  (2, 1, 2)\n",
      "\n",
      "====== Test 5 ======\n",
      "a_shape (2, 1, 2)\n",
      "b_shape (5, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 6 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (5, 1)\n",
      "Actual shape:  (2, 1, 5, 3)\n",
      "Torch shape:  (2, 1, 5, 3)\n",
      "\n",
      "====== Test 7 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (1, 5)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 8 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (1,)\n",
      "Actual shape:  (2, 1, 1, 3)\n",
      "Torch shape:  (2, 1, 1, 3)\n",
      "\n",
      "====== Test 9 ======\n",
      "a_shape (2, 1, 1, 3)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (2, 1, 5, 3)\n",
      "Torch shape:  (2, 1, 5, 3)\n",
      "\n",
      "====== Test 10 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (1, 5)\n",
      "Actual shape:  (5, 5)\n",
      "Torch shape:  (5, 5)\n",
      "\n",
      "====== Test 11 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (1,)\n",
      "Actual shape:  (5, 1)\n",
      "Torch shape:  (5, 1)\n",
      "\n",
      "====== Test 12 ======\n",
      "a_shape (5, 1)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (5, 3)\n",
      "Torch shape:  (5, 3)\n",
      "\n",
      "====== Test 13 ======\n",
      "a_shape (1, 5)\n",
      "b_shape (1,)\n",
      "Actual shape:  (1, 5)\n",
      "Torch shape:  (1, 5)\n",
      "\n",
      "====== Test 14 ======\n",
      "a_shape (1, 5)\n",
      "b_shape (5, 3)\n",
      "Custom implementation couldn't broadcast\n",
      "Torch couldn't broadcast\n",
      "\n",
      "====== Test 15 ======\n",
      "a_shape (1,)\n",
      "b_shape (5, 3)\n",
      "Actual shape:  (5, 3)\n",
      "Torch shape:  (5, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_are_broadcastable_together():\n",
    "    \"\"\"Tester for `are_broadcastable_together`\"\"\"\n",
    "    \n",
    "    testcases = [\n",
    "        ((1,), (1, 1)),\n",
    "\n",
    "        ((2, 1, 2), (2, 1, 1, 3)),\n",
    "        ((2, 1, 2), (5, 1)),\n",
    "        ((2, 1, 2), (1, 5)),\n",
    "        ((2, 1, 2), (1,)),\n",
    "        ((2, 1, 2), (5, 3)),\n",
    "\n",
    "        ((2, 1, 1, 3), (5, 1)),\n",
    "        ((2, 1, 1, 3), (1, 5)),\n",
    "        ((2, 1, 1, 3), (1,)),\n",
    "        ((2, 1, 1, 3), (5, 3)),\n",
    "\n",
    "        ((5, 1), (1, 5)),\n",
    "        ((5, 1), (1, )),\n",
    "        ((5, 1), (5, 3)),\n",
    "\n",
    "        ((1, 5), (1, )),\n",
    "        ((1, 5), (5, 3)),\n",
    "        ((1, ), (5, 3)),\n",
    "    ]\n",
    "\n",
    "    for i, (a_shape, b_shape) in enumerate(testcases):\n",
    "        print(f\"====== Test {i} ======\")\n",
    "        print(\"a_shape\", a_shape)\n",
    "        print(\"b_shape\", b_shape)\n",
    "        \n",
    "        a = torch.empty(a_shape)\n",
    "        b = torch.empty(b_shape)\n",
    "        \n",
    "        actual_broadcastable, actual_shape = are_broadcastable_together(a, b)\n",
    "\n",
    "        if not actual_broadcastable:\n",
    "            print(\"Custom implementation couldn't broadcast\")\n",
    "            \n",
    "        try:\n",
    "            torch_broadcast = torch.broadcast_tensors(a, b)\n",
    "        except:\n",
    "            print(\"Torch couldn't broadcast\\n\")\n",
    "            assert not actual_broadcastable, \"Torch couldn't broadcast but custom implemntation did\"\n",
    "            continue\n",
    "\n",
    "        torch_shape = tuple(torch_broadcast[0].shape)\n",
    "        print(\"Actual shape: \", actual_shape)\n",
    "        print(\"Torch shape: \", torch_shape)\n",
    "        \n",
    "        assert actual_shape == torch_shape, \"Torch and Custom implementation broadcasted differently\"\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "test_are_broadcastable_together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad7fc96f-ebb3-488c-babe-641e715c3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_tensors(a: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Basically check and get the shared size using `are_broadcastable_together` and then\n",
    "    invoke `expand` on `a` and `b`.\n",
    "\n",
    "    Returns\n",
    "        Tuple of:\n",
    "            {torch.Tensor} the expanded tensor emerged from `a`\n",
    "            {torch.Tensor} the expanded tensor emerged from `b`\n",
    "    \"\"\"\n",
    "    broadcastable, size = are_broadcastable_together(a, b)\n",
    "    if not broadcastable:\n",
    "        raise Exception(\n",
    "            f'Unable to broadcast tensors of shapes \"{a.shape}\" and \"{b.shape}\"')\n",
    "\n",
    "    return (\n",
    "        expand(a, size),\n",
    "        expand(b, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6992d85c-8275-477f-a51c-7b550c8c2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_broadcast_tensors():\n",
    "    testcases = [\n",
    "        ((1,), (1, 1)),\n",
    "\n",
    "        ((2, 1, 2), (2, 1, 1, 3)),\n",
    "        ((2, 1, 2), (5, 1)),\n",
    "        ((2, 1, 2), (1, 5)),\n",
    "        ((2, 1, 2), (1,)),\n",
    "        ((2, 1, 2), (5, 3)),\n",
    "\n",
    "        ((2, 1, 1, 3), (5, 1)),\n",
    "        ((2, 1, 1, 3), (1, 5)),\n",
    "        ((2, 1, 1, 3), (1,)),\n",
    "        ((2, 1, 1, 3), (5, 3)),\n",
    "\n",
    "        ((5, 1), (1, 5)),\n",
    "        ((5, 1), (1, )),\n",
    "        ((5, 1), (5, 3)),\n",
    "\n",
    "        ((1, 5), (1, )),\n",
    "        ((1, 5), (5, 3)),\n",
    "        ((1, ), (5, 3)),\n",
    "    ]\n",
    "\n",
    "    for i, (a_shape, b_shape) in enumerate(testcases):\n",
    "        a = torch.rand(a_shape)\n",
    "        b = torch.rand(b_shape)\n",
    "\n",
    "        error_custom = False\n",
    "        error_torch = False\n",
    "\n",
    "        try:\n",
    "            torch_broadcast = torch.broadcast_tensors(a, b)\n",
    "        except:\n",
    "            error_torch = True\n",
    "\n",
    "        try:\n",
    "            custom_broadcast = broadcast_tensors(a, b)\n",
    "        except:\n",
    "            error_custom = True\n",
    "\n",
    "        assert error_torch == error_custom\n",
    "\n",
    "        if not error_torch:\n",
    "            assert torch.equal(torch_broadcast[0], custom_broadcast[0]), \"First tensors of Torch and Custom implementation are not the same\"\n",
    "            assert torch.equal(torch_broadcast[1], custom_broadcast[1]), \"Second tensors of Torch and Custom implementation are not the same\"\n",
    "\n",
    "test_broadcast_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
