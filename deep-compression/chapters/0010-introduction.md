# Introduction

Neural networks (NNs) have passed great milestones over the last two decades and are now used to solve problems in a wide variety of fields. Over this period, the neural networks have greatly increased in size to support more complex problems and increasing amounts of data. That increase in size led to an increased amount of computation, storage requirements and power consumption.

In IoT and the edge, resources are limited. It is getting harder and harder to incorporate state-of-the-art models into such environments.

Outside the IoT and the edge, those state-of-the-art models utilize a tremendous amount of computation and power resources making them costly and pollutive. The increased amount of memory and computation resources require us to distribute the workloads and use hardware solutions such as GPUs, FPGAs and smart NICs - making the maintenance overhead high.

Neural network compression is the domain of reducing the size of a neural network while maintaining its accuracy up to a point. **Deep Compression** is a standard, three-phase framework to achieve this goal. Firstly, we prune the network by forcefully removing sub-structures within it - This process is called **Pruning**. Secondly, we share different parameters across different connections and reduce the granularity of the parameters' representations - This process is called **Quantization and Weight Sharing**. Lastly, we use **Huffman Coding** to compress the parameters. Other research has been made and different frameworks were proposed to compress neural networks.

In this work, we will provide an overview of different, mainstream Neural Network model compression approaches and illustrate some techniques for applying them. Finally, we will focus on a related hypothesis known as the "Lottery Ticket Hypothesis" made by *Jonathan Frankle*. As it was believed, pruning can only be performed on trained networks and that training smaller networks from scratch cannot yield high accuracies. The assumption was that learning requires more information than inferencing. In his hypothesis, Frankle claimed otherwise - he claimed that neural networks contain sub-networks within them that can be trained independently and still achieve similar accuracy as the original network. He calls those sub-networks "Winning Tickets" since finding them, is like winning the lottery.